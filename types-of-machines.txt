In the world of service deployment there are various strategies employed by network and resource providers to deliver a succinct product to the service developer. When a team develops a web application for example, people often ask first what front-end or server-side frameworks are being used, but how does this decision translate to the real world in production. We have heard of people developing server side business logic with suites such as Ruby on Rails or Django, but we don't often hear about the logistics behind deployment.

In reality there must be some way to serve business logic created by developers to end users who are potentially spread all around the world. For performance, bandwidthm and other obvious reasons, it is not the standard for people to run a service on a single machine, even if it was specifically configured for handling server loads. People typically choose to host their service with a larger organization such as Amazon or Microsoft that maintains physical machines and their access to the internet. Through this approach the programmer can worry solely about business logic and not neccesarily on scalability, a condition that really helps people develop and distribute software quickly. The problem of scalability and resource utilization is in this case kindly left for engineers who are well versed in the segmentation of physical machines between customers' services that demand their computational and network capabilities. The modern deployment market has solved this issue of sharing physical computing resources between independent service developers through two systems: virtual machines and containers. 

Virtual machines are configured to run alongside one another on a physical machine. They have their own operating systems, run their own services, and are for the most part completely invisible to one another. This allows a single machine to run a highly configurable environment and have access to a wide range of resources (bear in mind that the virtual machine can be started or moved to consume different amounts of resources). Virtual machines are a brilliant solution at first sight because they give complete configurability to the developer, irrespective of the physical machine(s) behind the scenes. Though highly configurable, there is some loss in the management and allocation of virtual machines since virtual disk space is difficult to reallocate and virtual machines are somewhat costly to swap between and manage. This sharing of a single physical machine could also potentially lead to situations where one service hogs all the network or disk capacity and makes it hard to run other deplyments alongside it.

Containers are a more malleable solution to the sharing of physical machines. The premise behind containers is that the machine will run a single kernel (typically a linux distribution or windows) and create logical 'containers' for services. The advantage over virtual machines is that the containers are highly portable across different physical machines, take less time to set-up and are able to share more of the same resources of the host machine, reducing waste.

Having either virtual machines or containers accessible in a cloud environment virtually eliminates the worry of scalability of service in development. Knowing that any framework can be set-up to run in different capacities and with a large scope of network access gives any project a huge leg up. For most developers looking to create a service for the greater internet, it is best to stick to the distributions available from companies like amazon, google and microsoft. For such a relatively little cost, it is possible to recieve the neccesary reach for pretty much all services. The only time a physical machine should be used to deploy a service aside from testing, is when the scope of the service is known in advanced to be relatively small or local, like a library database or minecraft server.
